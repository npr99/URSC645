# Reproducible

The table below consolidates the definitions of **reproducible** (and reproducibility) found across the readings in the [annotated bibliography](Readings\00_URSC645_AnnotatedBibliography.md)

> **Note to future students:** 
- Identify if the definition is a direct quote
- If it is not a direct quote check to see if the text can be replaced with a direct quote. Direct quotes are preferred over summaries.
- All direct quotes need to be inside " marks and include the page number (p. ###)
- Add page numbers for direct quotes or to the part of the text that is summarized.
- Check for accuracy current summary created using Claude Opus 4.6 

| In-Text Citation | Reproducible Defined |
| --- | --- |
| Freese (2007) | Reproducible refers to the ability to replicate or reproduce the results of a study using the same data and procedures. It involves making all necessary information, such as data, code, and materials, available to others to verify the presented results. |
| White et al (2013) | Reproducible is measured by the ability for existing scientific analyses to be reproduced and improved upon by others. |
| Gentzkow and Shapiro (2014) Ch. 1 | While not explicitly defined, reproducible is framed by its absence; it is hindered by code that no longer works or depends on manual steps that cannot be verified. |
| Gentzkow and Shapiro (2014) Ch. 2 | Defined operationally as the state where all output files (logs, figures, PDFs) can be deleted and then perfectly recreated by running a single execution script. |
| Goodman et al (2016) | (a) Methods reproducibility: the provision of enough detail about research procedures and data. (b) Results reproducibility: obtaining the same results from an independent study whose procedures are as close as possible. (c) Inferential reproducibility: drawing the same conclusions from an independent study or reanalysis of the original study. |
| Playford et al (2016) | Sharing research code is essential for achieving reproducibility, which enhances the accuracy and validity of analyses. A well-planned workflow and the sharing of research code are critical for improving the overall quality and efficiency of social science research. |
| Munafo et al (2017) | "Reproducibility… verification of research (including, for example, sharing data and methods)." |
| Munafo et al (2017) | Reproducibility is framed as a core scientific norm, defined as the ability to verify research findings through access to data, methods, and analytic decisions. |
| Christensen et al (2019) | The ultimate measure of a reproducible workflow is the "one-click workflow," where an entire analysis can be rerun from the ground up with a single command. The published article is merely "advertising"; the actual scholarship is the complete software environment and instructions that generated the results. |
| Frias-Navarro et al (2020) | Replication crisis: a systemic pattern in which a substantial proportion of published findings cannot be reproduced or replicated with comparable results. |
| Kontokosta (2021) | The ability to recreate or replicate the results of a study or analysis using the same data and methods. The processes, methodologies, and analyses conducted in a study are transparent and well-documented so that other researchers can replicate the findings. |
| Kedron and Holler (2026) | Reproducibility refers to obtaining consistent results using the same data and analytical procedures. |

### Summary: How Has the Definition of Reproducible Evolved?

Across the literature, the definitions of reproducible share a common core: **obtaining the same results using the same data and the same analytical procedures**. From Freese (2007) through Kedron and Holler (2026), nearly every author agrees on this basic premise. However, the concept has expanded and become more nuanced over time.

**Early definitions (2007–2014)** tend to be straightforward. Freese (2007) frames reproducibility as verification — duplicating results with the same data and procedures. Gentzkow and Shapiro (2014) operationalize it practically: if you can delete all output and recreate it by running a single script, the work is reproducible. At this stage, the definition is primarily about whether the *same researcher* (or someone with their files) can regenerate the results.

**A key turning point occurs with Goodman et al (2016)**, who argue that the term "reproducibility" is used inconsistently across disciplines and propose a three-part lexicon: (a) *methods reproducibility* (providing sufficient detail to rerun the analysis), (b) *results reproducibility* (obtaining the same results in an independent study with the same procedures), and (c) *inferential reproducibility* (drawing the same conclusions from either a reanalysis or an independent study). This is the most significant challenge to a single unified definition and highlights a genuine **disagreement** in the literature — what some authors call "reproducible," others call "replicable," and vice versa.

**Post-2016 definitions reflect this expanded awareness.** Munafo et al (2017) treat reproducibility as a broad scientific norm encompassing verification through shared data and methods, while Christensen et al (2019) push the operational definition further with the "one-click workflow" standard — the idea that the *entire* software environment and instructions constitute the actual scholarship, not just the published paper. This represents a shift from reproducibility as a *property of results* to reproducibility as a *property of the workflow itself*.

**More recent work (2020–2026)** tends to settle on a distinction that has gained traction: *reproducibility* means same data + same methods = same results, while *replicability* means new data + same methods = consistent results (Kedron and Holler, 2026). Frias-Navarro et al (2020) use the terms somewhat interchangeably when describing the broader crisis, but the field appears to be converging on clearer distinctions.

**Key areas of agreement:**
- Reproducibility requires transparency — sharing data, code, and documentation
- Reproducibility is essential for scientific credibility
- Scripted, automated workflows are preferable to manual, undocumented steps

**Key areas of disagreement or evolution:**
- Whether "reproducible" and "replicable" mean the same thing (Goodman et al 2016 explicitly argues they do not)
- Whether reproducibility is a property of the *results* or of the *entire workflow and environment* (Christensen et al 2019 argues the latter)
- The scope of what must be shared — early definitions focus on data and code, while later definitions expand to include the computational environment, software versions, and documentation of analytic decisions

In summary, the definition has not fundamentally changed, but it has **deepened**. What began as a simple expectation — "can someone get the same numbers?" — has evolved into a more demanding standard that encompasses methods transparency, computational environments, and the full research lifecycle.

----
AI Use Statement: 

This page was generated on **February 10, 2026** using **GitHub Copilot (Claude Opus 4.6)** through a **VS Code GitHub Copilot Pro Education License**.

The following prompts were used in sequence:

1. **Extract and tabulate definitions:** "Review this file. I would like to consolidate the definitions of the terms reproducible into a table at the bottom of the file. Make a section title Reproducible and then a table with one column with the in text citation for the paper (Munafo et al 2017) then a second column with the definition that was found. Provide a third column for the page number (this column should be empty) - make a note that future students will need to add the page numbers."
2. **Sort the table:** "This is great. Please sort the list by year and then author last name."
3. **Summarize and analyze definitions:** "Perfect. Create a follow up section that summarizes the definitions. Are there any disagreements or has the definition changed over time?"
4. **Add AI use statement:** "Add an AI use statement to the bottom of this page. Describe that this was generated using VS Code GitHub Pro Education License on the date and summarize the prompts used."

The AI reviewed the full contents of [00_URSC645_AnnotatedBibliography.md](00_URSC645_AnnotatedBibliography.md) to identify and extract definitions of "reproducible" and "reproducibility" from each article entry, then consolidated them into the table above. The summary section synthesizing agreements, disagreements, and evolution over time was also generated by the AI based on the extracted definitions.