### Where to start?
- [Contribute to the course Google Doc](https://docs.google.com/document/d/1elFspZUIEzc66LLatNqcZBW3WM7vJ7qKHhX6Z0YcEVo/edit?usp=sharing)
- Use Google Doc to Markdown Extension to update this page

### Hello World from Jiang

# Annotated Bibliography

## Introduction

This annotated bibliography highlights the following ideas and concepts as discussed in the literature:

1. Definitions of terms such as reproducible, replicable, workflow, science
2. Challenges associated with reproducible research
    - Social norms vs individual goals - tradeoffs
3. Computer coding, scripting, and programming concepts
4. Appropriate software for replicable research
5. Discussions of bias (what is better science?)

## The Problem

Research is a fundamental part of science that involves a significant investment of time, systematic application of methods, and careful consideration. 
"Science demands reproducibility" (Long 2009, p. 2); therefore, reproducible research is a fundamental part of science. 
However, most research is not reproducible because many of the research methods are not clearly documented, and tools or software are not always available.
Additionally, within the social sciences, there is a general sense while research should be reproducible, the requirements are left to individual researchers.
To overcome the gap between the demand for reproducible research and the lack of skills to build reproducibility into projects, researchers need to invest time to develop data science skills. Data science skills can help build a strong workflow to support urban and regional analytic research that is systematic, generalizable, and replicable. 
Researchers who invest time to develop skills for reproducible research will be able to accomplish better science in less time.

## Research Question:

_How do reproducible workflow skills affect the quality and efficiency of science?_

The research question defines the sections to focus on in the literature review. Within each article review look for definitions and ways to measure the following terms:
1. reproducible
2. workflow 
3. workflow skills
4. quality of science
5. efficiency of science

## Audience

This annotated bibliography is for academic researchers interested in reproducible research.

---
# Required Reading

## Munafo et al 2017
Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Percie du Sert, N., Simonsohn, U., Wagenmakers, E.-J., Ware, J. J., & Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(0021). https://doi.org/10.1038/s41562-016-0021

Problem addressed:
The article discusses the reproducibility crisis in scientific research, which is caused by issues such as low statistical power, publication bias, P-hacking, and lack of transparency practices. These factors undermine the robustness, credibility, and efficiency of the scientific process.

The manifesto calls for systemic changes across the scientific ecosystem, including:
1. Researchers adopt rigorous methodological standards.
2. Institutions, journals, and funders incentivize transparency and reproducibility.
3. Conduct collaborative and multi-site studies to improve generalizability and effectiveness.

Limitations: Implementation may vary across disciplines, and financial/resource constraints may hinder adoption.


*   Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [http://www.prisma-statement.org/](http://www.prisma-statement.org/) 
*   Transparency and Openness Promotion Guidelines (TOPS) [https://www.cos.io/initiatives/top-guidelines](https://www.cos.io/initiatives/top-guidelines)

**Definitions:**

“A hallmark of _**scientific**_ creativity is the ability to see novel and unexpected patterns in data.” (Munafò et al., 2017, p. 1)

“Open _**science**_ refers to the process of making the content and process of producing evidence and claims transparent and accessible to others.” page 5

“_**Reproducibility**_… verification of research (including, for example, sharing data and methods).” p. 5

## Freese 2007

Freese, J. (2007). Replication standards for quantitative social science: Why not sociology?. _Sociological Methods &amp; Research_, _36_(2), 153-172.

Freese defines replicate as " the simple verification or duplication of results using the same data and analytic procedures as the original researcher." (p. 154) Freese points out within Sociology there is an implicit individualistic policy that all published research should be replicable. Freese argues that the responsibility to do replicate research should be based on a social contract.

| Individual policy | Social policy |
| --- | --- |
| If asked a researcher should be able to provide the files to replicate published results | Files to replicate published results are submitted at time of publication. |
| emphasis on trust over transparency, faith in the author, reinforcement of status, ethically obscure, expires with the individual | Reduce egalitarianism. Distributes power and access. Openness. Highlights exemplary work. |

More recent examples:

_American Sociological Review_ ([https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines](https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines))

- ASA's stated policy on data-sharing: "Sociologists make their data available after completion of the project or its major publications, except where proprietary agreements with employers, contractors, or clients preclude such accessibility or when it is impossible to share data and protect the confidentiality of the data or the anonymity of research participants (e.g., raw field notes or detailed information from ethnographic interviews)" (ASA Code of Ethics, 1997)

_Social Psychology Quarterly_ (In Association with American Sociological Association)

([https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines](https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines))

- SPQ encourages researchers to make their data, code, and other materials available for others to review and use. We are happy to assist authors in locating a suitable depository for these materials and will be pleased to add footnotes pointing to these deposits in the published article. Because we know that not all areas of social psychology are able to provide these materials, practically or ethically, this is not a requirement and willingness to publicly release data and other materials will have no impact on the likelihood of acceptance. (Revised January 2018)

Elsevier Journals: research data sharing webpage (https://www.elsevier.com/authors/author-services/research-data)

"A number of Elsevier journals encourage authors to submit a data statement alongside their manuscript." "Several journals support co-submission of a data article alongside your manuscript. In each case, the data article is transferred to the open access journal _Data in Brief_ for publication."

Taylor &amp; Francis offers the following standardized data sharing policies across our journals ([Journal of the American Planning Association included):](https://www.tandfonline.com/toc/rjpa20/current)

([https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/](https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/))

From 2018, Taylor &amp; Francis will be introducing new policies on data sharing….

- Basic – Journal encourages authors to share and make data open where this does not violate protection of human subjects or other valid subject privacy concerns. Authors are further encouraged to cite data and provide a data availability statement.
- Share upon reasonable request – Authors agree to make their data available upon reasonable request. It's up to the author to determine whether a request is reasonable.
- Publicly available – Authors make their data freely available to the public, under a license of their choice.
- Open data – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Data shall be findable and fully accessible.
- Open and fully FAIR (Findable, accessible, interoperable and re-usable) – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Additionally, data shall meet with FAIR standards as established in the relevant subject area.

## Lowndes et al 2017

Lowndes, J. S. S., Best, B. D., Scarborough, C., Afflerbach, J. C., Frazier, M. R., O’Hara, C. C., Jiang, N., & Halpern, B. S. (2017). Our path to better science in less time using open data science tools. Nature Ecology & Evolution, 1(6), 0160. https://doi.org/10.1038/s41559-017-0160 

### Overview
<p align="justify">
This paper provides three essential concepts and discussions related to our research question. First, it explores how replicability and open data science improve the quality of research. Second, what workflows can contribute to replicability? Third, how to learn open data science and achieve the transformation from traditional research approaches to an open and transparent research environment.
</p>
1. Benefits of Replicability
<p align="justify">
Replicability in research enhances confidence in findings, promotes transparency, and improves efficiency by reducing redundant efforts. It fosters collaboration, improves research quality, and bridges science with policy. By supporting open science, replicability ensures accessibility and reliability, ultimately accelerating scientific progress and the practical application of research.
</p>
2. Workflows and Practices for Reproducibility
<p align="justify">
Adopting open data science tools, such as R, Git, and GitHub, enhance reproducibility by organizing, documenting, and sharing data. Standardizing formats, coding data preparation, and using version control improve collaboration and efficiency. Openly sharing code, structuring files consistently, and using platforms like GitHub Issues facilitate communication, and finally, follow open science principles—transparency, collaboration, and sharing.
</p>
3. Learning Open Data Science and Transformation
<p align="justify">
Learning strategies include self-paced resources, community engagement, workshops, and incremental adoption of tools like R and GitHub. Overcoming challenges requires patience, teamwork, and confidence in coding. Good teamwork values coding skills, fostering leadership, and sharing knowledge. Ultimately, through the open sharing of data, code, and methods, moving from traditional, often inefficient, and less reproducible methods to a more open, collaborative, transparent, and efficient approach using data science tools and principles.
</p>

### Critical Citation
"Integrating open data science practices and tools into science can save time, while also improving reproducibility for our most important collaborators: our future selves." - Lowndes et al

Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O'Hara, Ning Jiang and Benjamin S. Halpern (2017). Our path to better science in less time using open data science tools. _Nature Ecology &amp; Evolution_, 1, 1-7.

### Introduction

Lowndes provides a methodological account of the open data tools that they use to improve their workflow (i.e. data preparation, modeling, coding, collaboration and sharing data).

### Argument

It is often implied that good science is science that is reproducible. One of the main challenges across disciplines is making data accessible, transparent and collaborative. The author introduces various open data science tools that can make the process of reproducibility and collaboration easier. Shifting from an individual to collaborative approach to data analysis is beneficial by allowing for analysis and interpretation to become a collaborative learning process resulting in individual and collective growth and improvement.

### Audience

An experienced community of data analysts and researchers interested in collaboration and improving methodology through collaboration.

### Content

Lowndes walks us through a number of methods and tools that her and collaborative research partners use to improve their workflow and the reproducibility of their work. Table 1 provided a summary of workflow tasks, primary open data tools and how tools have advanced to make workflow more efficient. For example, every time R Markdown output files are regenerated, the R code is rerun and the associated text and figures are also regenerated to reflect any updates to the code. Another resource they used was Git 44 as a version control system tool (which tracks changes within files and allows you to revert to previous version if needed). It takes snapshots of the changes being made line-by-line and throughout time.

1. Definition not mentioned.
2. Software usually creates barriers between researchers and restrains data sharing, because of the cost of software researchers use in their research. Further, individual data organizations place challenges in reproducing research. Workflow and sharing data organization is essential to overcome replicable research challenges.
3. Free software helps establish workflow and sharing data. Establishing workflow and sharing data improve data replicability, which cannot be based on individual data organization.
4. How free software tools and platforms can help collaborative research, improving workflow, and replicability. It uses an Ocean Health Index project as an example of replicability and collaboration research.Open sources also help reproducibility.
5. Emphasis on reproducibility, collaboration, and communication

### Argument:

Free software tools and platforms, like Github, can help collaborative research, improving workflow, and replicability, because it breaks the barriers between researchers.


## Gentzkow and Shapiro (2014)
Gentzko, M., & Shapiro, J.N. (2014). Code and Data for the Social Sciences: A Practitioner’s Guide. https://web.stanford.edu/~gentzkow/research/CodeAndData.pdf

Purpose of book: To share "insight from experts in code & data into practical terms." The whole book focused on workflow skills. 

### Chapter 1: Introduction
This chapter introduces the challenges of empirical social science, emphasizing the central role of coding and data management. Many researchers lack formal computer science training, leading to common issues like non-replicable results and difficulty in understanding old code. The authors describe common situations they encountered, such as code that no longer works and inconsistent results. The chapter argues for learning from experts in code and data management and presents the guide as a means of translating these practices into the social sciences to improve research workflows. The main goal of this guide is to help researchers focus on the research questions rather than wrestle with messy code.

### Chapter 2: Automation
This chapter stresses the importance of automating all research steps. It contrasts an inefficient interactive approach to research with a more structured, script-based approach. The authors argue for the use of scripts to make research more replicable and efficient. The chapter provides an example of converting an interactive workflow into a set of scripts and then demonstrates how to make the output of the project replicable by creating a single script that executes all code. The chapter concludes by emphasizing the benefits of automation in terms of time and effort savings.

### Chapter 3: Version Control
This chapter introduces the concept of version control to manage code and data changes. It argues against using methods like date and initial tags and instead advocates for using version control software to track changes automatically. The chapter highlights the benefits of version control, including the ability to revert to previous versions, record authorship, and see detailed changes. The authors emphasize the importance of running the entire project before checking in changes to ensure the code is working properly and consistently. This practice ensures replicability and makes editing less risky.

### Chapter 4: Directories
This chapter focuses on the organization of project directories to improve efficiency and clarity. It recommends separating directories by function and separating input files from output files. The authors suggest a directory structure that includes subdirectories for inputs, outputs, code, and temporary files. The chapter explains how modular, functional directories facilitate modifications, make data dependencies clear and allow easy access to the output of specific directories from any other directory. The authors suggest that making directories portable allows them to be run on different machines.

### Chapter 5: Keys
This chapter discusses the importance of storing cleaned data in tables with unique, non-missing keys. The authors use an example to illustrate common issues, such as missing or conflicting data. They introduce the concept of a relational database, and emphasize that the physical structure of a database should communicate its logical structure. The guide stresses that each table should have a key to identify its elements and that data should be normalized to avoid ambiguity. The chapter concludes with a three-step process to organize data from raw sources to the final analysis.

### Chapter 6: Abstraction
This chapter discusses the concept of abstraction in coding to eliminate redundancy and improve clarity. The authors provide an example of calculating a leave-out mean to demonstrate the problem of using copy-and-paste methods and offer a solution by creating a general-purpose function. They also suggest that abstraction makes the code more readable. The chapter emphasizes the importance of implementing functions carefully, including unit testing to ensure functions work as intended. The authors also highlight the use of abstraction beyond code in everyday tasks.

### Chapter 7: Documentation
This chapter addresses the importance of documentation, advocating for self-documenting code over extensive comments that can be difficult to maintain. The authors describe the issues with comments that contradict the code. They argue that code should be written so that its structure and variable names guide the reader. They suggest using comments to clarify when something may seem wrong at first. Additionally, comments can also be used to prevent unintended behavior by clarifying the input required for functions.

### Chapter 8: Management
This chapter focuses on task management, stressing the importance of using a task management system instead of email. The authors illustrate the ambiguity and confusion that can result from relying on email to manage tasks. They recommend using project and task management systems that provide structure and clear communication. They give an example of a task management portal and highlight the benefits of task management systems, such as clear responsibilities and a centralized place for all communications. They also recommend using a collaborative note-taking environment to organize notes and share them with other users.

### Appendix: Code Style
This section outlines principles for writing good code, emphasizing that code has multiple audiences, including the computer, the author, and collaborators. The authors recommend keeping code short and purposeful and ordering functions for linear reading. The chapter also promotes descriptive names for variables, functions, and files, highlighting the importance of consistency. The authors also stress the importance of error checking and testing and advocate profiling slow code to identify areas of improvement. The chapter concludes by discussing the tradeoff between storage and CPU time and suggests that slow and fast code should be separated.

**Principles of writing good code:**
1. Keep it short and purposeful.
2. Make your functions shy.
       a. Functions should explicitly declare inputs and outputs and only operate on local variables.
3. Order your functions for linear reading.
4. Use descriptive names.
5. Pay special attention to coding algebra.
    a. Make sure that key calculations are clearly set off from the rest of the code.
    b. Break complicated algebraic calculations into pieces.
6. Make logical switches intuitive.
7. Be consistent.
8. Check for errors.
9. Write tests for just about every piece of code to make sure that the code does everything it is expected to do.
10. Profile slow code relentlessly.
11. Store “too much” output from slow code.
12. Separate slow code from fast code.

## Goodman et al. (2014) Ten Simple Rules for the Care and Feeding of Scientific Data

<p align="justify">
Goodman A, Pepe A, Blocker AW, Borgman CL, Cranmer K, Crosas M, et al. (2014) Ten Simple Rules for the Care and Feeding of Scientific Data. PLoS Comput Biol 10(4): e1003542. https://doi.org/10.1371/journal.pcbi.1003542
</p>
<p align="justify">
This paper emphasizes the importance of data management and data care in terms of reproducibility and the quality of research. In the meanwhile, it provides some data management practices to facilitate reproducibility.
</p>
<p align="justify">
1. Effective data management ensures research reproducibility, transparency, and long-term value. It promotes data reuse, facilitates collaboration, and enhances credit and recognition for researchers. By preserving data and providing detailed workflows, it addresses the complexities of modern research, ensuring the reliability, validity, and accessibility of scientific findings.
</p>
<p align="justify">
2. Sharing data with persistent identifiers, providing metadata, documenting workflows, publishing code, and linking data to publications are essential for reproducibility and reuse. Using standard formats, version control, workflow management systems, and packaging research materials enhance collaboration and long-term accessibility. Open licenses and data papers ensure proper credit and recognition.
</p>


## Goodman et al 2016

Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. (2016). What does research reproducibility mean?. _Science translational medicine_, _8_(341), 341ps12-341ps12.

1. The paper introduces a new lexicon for research reproducibility. (a) Methods reproducibility, the provision of enough detail about research procedures (e.g. how many analyses were performed) and data. (b) Results reproducibility refers to obtainings the same results from a conduct of an independent study whose procedures are as close as possible. (c) Inferential reproducibility refers to drawing the same conclusions from an either independent study or a reanalysis of the original study.
2. The article points out some of its basic terms - reproducibility, replicability, reliability, robustness, and generalizability - are not standardized, leading to both conceptual and operational confusion.
3. Not mentioned
4. Not mentioned
5. Conclusion: Ill-defined reproducibility can be improved by a clear specification of reproducibility (methods, results, or inferences) and how it affects knowledge.

### Argument

The researchers need to move toward a better understanding of the relationship between reproducibility, cumulative evidence, and the truth of scientific claims.

## Long (2009) - The Workflow of Data Analysis Using Stata

### Chapter 1 & 2
This book emphasizes that a **good workflow is essential for the replication of research, which is a cornerstone of good science**. The author argues that researchers often develop workflows ad hoc, learning from mistakes. Chapters 1 and 2 introduce a structured approach to data analysis, highlighting the importance of planning, organization, and documentation (PO&D).  **Replicability is the guiding principle**, and the book stresses that workflow should facilitate this. **Improved workflow leads to greater efficiency, accuracy, and the avoidance of errors**, ultimately improving the quality of scientific work.

### Chapter 3

Problem: In this chapter, Long (2009) discusses the importance of writing and debugging do-files. Each of the tools he provides contribute to the accuracy, replicability, and efficiency of your work. While there may be a time investment in learning these tools, it will quickly be recovered as they are applied to the work being done. When conducting research, we often run into errors. We must know how to diagnose any arising issues. Long argues that the most effective way to submit commands is with do-files, which is a primary form of documentation.

The primary research question in this chapter:
- Can you (or how do you) run a do-file without running into any errors and producing the same results?

Definitions:

Do-file: Do-files are simply text files that contain your commands (Long, p. 49)

Robust: A do-file that produce exactly the same result when run at a later time or on another computer (Long, p. 51)

Legible: A do-file that is documented and formatted so that it is easy to understand what is being done (Long, p. 51)

Long suggests that the reproducibility can be measured by the following:
- Understanding how to execute commands
- Writing effective do-files
- Debugging do-files

In previous chapters, long has emphasized the importance of a good workflow. In this chapter, he states that “part of an effective workflow is taking advantage of the powerful features of your software” (Long, p. 47).

Some of the workflow skills discussed relate to writing effective do-files. More specifically, he suggest that do-files should be robust and legible. There are detailed instructions on how to make sure they fit this criteria  (Long, p. 51).

The quality of science can be assessed by ensuring the quality of do-files and knowing how to debug them in case of errors.

For the efficiency of science, he suggests having templates for do-files. “The more uniform your do-files are, the less likely you are to make errors and the easier it will be to read your output (Long, p. 63).

The Command Window: Long goes into detail about the interface of Stata and gives suggestions for how to execute commands, such as the review window, variables window, etc. (Pg. 48). One way to do that is with dialog boxes. These are drop down functions in the main Stata interface that allow you to execute commands. However, he highly suggests executing commands with a do-file. He states that 99% of the work he does in Stata uses do-files (Long, p. 49). The importance of this is that 1) it keeps a record of the commands  you ran, and 2) you can use features of your text editor, including copying, pasting, global changes, and much more.

In order to replicate and correctly interpret results, they must be robust and legible (Long, p. 51).

To make a do-file robust, they must meet the following:
- Need to be self-contained – it should use the dataset that is loaded. The do-file should not rely on something left in memory by a prior do-file or command.
- Use version control – this is done to ensure a newer version of Stata computes the commands correctly (e.g., you run a do-file that was created on an older Stata version)
- Exclude directory information – this is done so that you can run do-files on other computers without any changes since data will not always stay in the same place (you will possibly move the data over the years)

To make a do-file legible, Long suggest the following:
- Use lots of comments. For example, if you start a line with *, everything that follows is treated as a comment. You can also use // and comments as dividers.
- Use short lines. Having lines (e.g. commands) that are too long cause problems.
- Limit your abbreviations
- Have command abbreviations (see image below)
- Most importantly, be consistent! You will make fewer errors if you have a standard way to do things. This applied to the style of your do-files. 

![image](https://github.com/jason805sm/URSC645/assets/158380929/290b2ebd-7535-4433-8f85-61d43bc61053)

One of the more important suggestions Long makes it to have a template for do-files (Long, p. 63). He provides suggestions for things that should be included in your template from creating a simple do more to a more complex one (Long, p. 65).

In practice, your do-files will run into errors. It is imperative that we have solutions for how to debug them (Long, p. 68). Long provides instructions for fixing simple errors in do-files and how to fix them: it is likely that the a0 log file is open, b) log file already exists, c) incorrect variable name, d) incorrect command name, e) incorrect option, or f) missing comma before options.

He also provides steps for resolving errors (Long, p. 70). He gives general strategies that we should consider if we do not see an obvious solution for the error code encountered: Step 1 is to update Stata and user written programs (including the do-file). Step 2 is to start with a clean slate, which consists of not leaving any information in the memory, restarting Stata if necessary, rebooting your computer, etc. (Long, p. 71). Step 3 is to try other data. Step 4 is to assume everything could be wrong. Step 5 is to run the program in steps. Step 6 is to exclude parts of the do-file. Step 7 is to start over (e.g., throw out the original code, use a new file). Step 8 is to know that sometimes it is not your mistake – it could be possible that there is an error in Stata.

Long also provides instructions on debugging syntax errors (Long, p. 75, unanticipated results (Long, p. 77), and more advanced methods for debugging (Long, p. 81). If nothing else works, trace the error. His last suggestion is to ask for help so that another person try and replicate the work and find a solution (Long, p. 82).

## Hoelter, L. F., LeClere, F., Pienta, A. M., McNally, J. W., & Barlow, R. E. (2008). Using ICPSR Resources to Teach Sociology. Teaching Sociology, 36, 17-25.

Article can be found here: https://drive.google.com/file/d/1hjta3dVpWkgRUpiJp4VuQlmOh3NOdAXz/view?usp=drive_link

Problem: Over the last two decades, colleges and universities have taken on the challenge of teaching quantitative literacy (QL) to students in all majors. The social sciences are “particularly well suited to teaching students these skills” because can engage in problem solving with data
(Hoelter et al., p. 17). However, there are challenges to include quantitative literacy in undergraduate classes, such as re-working syllabi and assignments. It is also the case that students are resistant to group work, and faculty and students sometimes have difficulty separating QL from traditional mathematics as they both evoke similar stresses and anxieties. The authors suggest several ways in which the Inter-university Consortium for Political and Social Research (ICPSR) and its resources can enhance students’ quantitative literacy in coursework and across the undergraduate and graduate curricula. They also discuss several general tools available through the ICPSR website to enhance teaching and research at all levels.

The primary research question in this article:

What are the tools available to aid in the development of a curriculum that teaches undergraduate and graduate students quantitative and statistical literacy skills early in their academic careers?

Definitions:

Quantitative Literacy: The ability to understand and critically evaluate statistical results that permeate our daily lives – coupled with the ability to appreciate the contributions that statistical thinking can make public and private, professional and personal decisions (Hoelter et al., p. 18).

Information Literacy: Finding and critically evaluating information to answer questions

Statistical Literacy: Using statistics as evidence for arguments

Data Literacy: The need to understand and work with data

Workflow: Not found in this article.

Workflow skills: The authors point out several tools available for students to engage in Quantitative Literacy (QL) skills early, such as those found on the ICPSR website. Specifically, using Online Learning Center for undergraduates and Secondary Datasets for graduate students are some ways to teach workflow skills.

Quality of science: In the article, the authors state that asking students to replicate an article of interest in sociology or other fields is a valuable way to socialize students to the expectations and practices of the discipline, teaches them the norms of scientific transparency, and helps them get published early in their careers (Hoelter et al. p. 21).

Efficiency of science: Using the online tools suggested helps students to begin direct replication practices by allowing them to ask research questions that they are interested in and finding an article in a sociological journal of their interest (Hoelter et al. p. 21).

Goals, Resources, and Tools: ICPSR has prioritized making data holdings more accessible to faculty and students. When interviewing faculty in focus groups and in-depth interviews, they specified wanting the ability to 1) quickly locate relevant data that are easy to work with to demonstrate the content(s), and 2) customize any material to their own teaching approach and syllabus. This resulted in the creation of the Online Learning Center (OLC).

The OLC is a teaching/learning module that helps identify relevant, usable datasets quickly. The core of the site is made up of Data-Driven Learning Guides (DDLG’s) - it is set up like a lesson plan and makes use of “real” data from ICPSR holdings. The benefits of this are that no license is required nor do students need to learn statistical software.
 
A New Approach to Teaching Research Methods: The exposure to the scholarly process outside of some isolated journal articles is why students may have a problem coming up with good research questions that lead to the analysis of quantitative data (Hoelter et al., p. 20). Authors suggest a more useful way: “to use a model in which the students analyze not data in the traditional sense but the objects of social science discourse: the scholars, datasets, and publications that compose a network of scholarly exchange” (Hoelter et al., p. 20). A tool called Exploring Data through Research Literacy (EDRL) is available for this.

The Use of Secondary Data in Graduate Courses: Authors contend that asking students to replicate an article of interest is a valuable way to socialize students to the expectations and practices of the discipline (of sociology), and it teaches them the norms of scientific transparency and helps them get published early in their careers (Hoelter et al., p. 21). For this, there are two tools available: 1) the Publication-Related Archive, and 2) the Bibliography of Data-Related Literature.

![image](https://github.com/jason805sm/URSC645/assets/158380929/4d029dfc-ca84-4b02-8609-6f9290e991bc)

General Tools for Making Data Instruments and Analysis Easier: Another tool for students is the Survey Documentation and Analysis Interface (SDA), which was developed at UC Berkeley. There is also a variable search tool, as well as a Quick Tables to explore datasets and variables. Tutorials and teaching aids can be found throughout the ICSPR website. To foster interest in data analysis, students are awarded an Undergraduate Paper Competition for completing a research project. Those who want to learn more can apply for a summer internship at ICPSR.


## Goodman et al 2016
Goodman, S. N., Fanelli, D., & Ioannidis, J. P. (2016). What does research reproducibility mean?. Science translational medicine, 8(341), 341ps12-341ps12.

The paper introduces a new lexicon for research reproducibility. (a) Methods reproducibility, the provision of enough detail about research procedures (e.g. how many analyses were performed) and data. (b) Results reproducibility refers to obtainings the same results from a conduct of an independent study whose procedures are as close as possible. (c) Inferential reproducibility refers to drawing the same conclusions from an either independent study or a reanalysis of the original study.
The article points out some of its basic terms - reproducibility, replicability, reliability, robustness, and generalizability - are not standardized, leading to both conceptual and operational confusion.
Not mentioned
Not mentioned
Conclusion: Ill-defined reproducibility can be improved by a clear specification of reproducibility (methods, results, or inferences) and how it affects knowledge.
Argument
The researchers need to move toward a better understanding of the relationship between reproducibility, cumulative evidence, and the truth of scientific claims.

Want to convert another document?

Feedback
Source
Donate
Terms
Privacy
@benbalter


## The Turing Way: A Handbook for Reproducible Data Science 

The Turing Way Community, Becky Arnold, Louise Bowler, Sarah Gibson, Patricia Herterich, Rosie Higman, … Kirstie Whitaker. (2021, Nov 10). The Turing Way: A Handbook for Reproducible Data Science (Version v1.0.1). Zenodo. http://doi.org/10.5281/zenodo.5671094 Ebook link: https://the-turing-way.netlify.app/welcome

---
# Additional Resources

## Healy 2019 Plain Text Social Science
Healy 2019 Plain Text Social Science https://kieranhealy.org/files/papers/plain-person-text.pdf

Looks like an excellent resource - need to read and include here

## ICPSR Data Prep Guide
ICPSR (v6) Guide to Social Science Data Preparation and Archiving https://www.icpsr.umich.edu/files/deposit/dataprep.pdf

Inter-university Consortium for Political and Social Research (ICPSR)

## Center for Open Science
Transparency and Openness Promotion (TOP) Guidelines retrieved from https://www.cos.io/initiatives/top-guidelines

## Netflix 2019: Explained: Coding
Netflix (Oct 24, 2019) Explained: Coding https://www.imdb.com/title/tt11167964/

Covers history, abstraction and introduces machine learning and bias

## Gandrud 2018: Journal articles are advertising not research
Gandrud, C. (2018). Reproducible research with R and RStudio. Chapman and Hall/CRC. [Ebook at TAMU Library](http://proxy.library.tamu.edu/login?url=https://ebookcentral.proquest.com/lib/tamucs/detail.action?docID=4710300)

"Slideshows, journal articles, books... are the "advertising"" (p. 3)
The research is the "full software environment, code, and data that produces the results." (Buckheit and Donoho, 1995; Donoho, 2010, 385) - In Donoho quote is attributed to Claerbout 1992

Look up:
Donoho, D. L. (2010). An invitation to reproducible computational research. Biostatistics, 11(3), 385-388.  https://doi-org.srv-proxy1.library.tamu.edu/10.1093/biostatistics/kxq028

Claerbout J,  Karrenbach M. Electronic documents give reproducible research a new meaning. In: Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics, 1992(pg. 601-604)

Donoho, D. (2017). 50 years of data science. [Journal of Computational and Graphical Statistics](http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf), 26(4), 745-766.


## White, E.P., Baldridge, E., Brym, Z.T., Locey, K.J., McGlinn, D.J., & Supp, S.R. (2013). Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1-10. doi:10.4033/iee.2013.6b.6.f

**Audience**: Anyone creating datasets; but specific examples are for ecology & evolutionary biology scientists

**Purpose of Article**: To provide tips, recommendations, and resources on how to make your data more shareable for others

**Software**: Python & R; some other recommendations

**Key Definitions**\*:  
Reproducible - no definition  
Replicable - no definition  
workflow - no definition  
quality of science - no definition  
efficiency of science - no definition  
\*The authors provide no key definitions of words found in the article, however, the entire article is devoted to the importance of quality dataset for the purpose of reproducing, replicating, and reusing data.

**Summary**:  
_Introduction_  
Data sharing is becoming a more widely recognized important element of the scientific process for two reasons: “allowing the replication of research results” and the “reuse of meta-analyses and projects not.. intended" by the original data collectors (pg1). However, even if data sharing occurs more, the impact of data sharing will be limited if the shared data is unusable due to structure, metadata, or licensing issues.

_Tip 1 - Share your data_  
Authors note that individuals are reluctant to share their data for a variety of concerns, such as competition over publications from the dataset, lack of giving credit where credit is due, and the idea that sharing data is challenging and time intensive. The authors counter that there are two developments that address these concerns: 1) there are limitations or embargos on data sharing to address the concerns regarding competition and 2) datasets are now citable.

The authors identify four ways sharing data is beneficial to the science community:

*   “(1) the results of existing analyses to be reproduced and improved upon (Fienberg and Martin 1985, Poisot et al. 2013)
*   (2) data to be combined in meta-analyses to reach general conclusions (Fienberg and Martin 1985)
*   (3) new approaches to be applied to the data and new questions asked using it (Fienberg and Martin 1985)
*   (4) approaches to scientific inquiry that could not be considered without broad scale data sharing (Hampton et al. 2013).” (pg 2).

They also discuss how on an individual level, sharing data is beneficial, because it can increase your citation count and allows you to reuse the data in new ways in the future.

_Tip 2 - Provide Metadata_  
The authors define metadata as “information about the data, including how it was collected, what the units of measurement are, and descriptions of how to best use the data (Michener and Jones 2012)” (pg2). Metadata is important, because it provides clarity on how useful a dataset will be for a specific project, it makes the dataset easier to use, and it makes the dataset useful for longer.  

Good meta data should include:

*   “The what, when, where, and how of data collection.
*   How to find and access the data.
*   Suggestions on the suitability of the data for answering specific questions.
*   Warnings about known problems or inconsistencies in the data, e.g., general descriptions of data limitations or a column in a table to indicate the quality of individual data points.
*   Information to check that the data are properly imported, e.g., the number of rows and columns in the dataset and the total sum of numerical columns.” (pg 2)

Metadata can come in many forms, but should be "logically organized, complete, and clear enough to enable interpretation and use of data." (pg 2). The authors recommend following the metadata standards of your field, because that will help with the reusability of your dataset. Developing good metadata starts with describing your data in the planning and collection stages.

Resources for better Metadata

*   KNB Morpho
*   USGS xtme
*   FGDC workbook

_Tip 3 - Provide an unprocessed form of data_  
It is important to provide your data in its most raw form and the clean set you used. The two datasets provide future users with more flexibility. Explain the difference between the two sets in the metadata.

_Tip 4 - Use standard data formats_  
The authors suggest using a standard format to make it easier for others to use your data. Good standards include: File type, overall data structure, and file content.

4.a - Use standard file formats  
The authors suggest using a file format that is readable by most software and when possible is non-proprietary. Additionally, it should be stored in a text file, because proprietary software like Microsoft Excel can be challenging to load in other programs. It can also become old, no longer open and therefore make your data inaccessible in the future.

Additionally, the authors suggest using tables when the data doesn't have a well defined format. Naming files should be consistent, descriptive, and no spaces.

4.b - Use standard table formats  
The authors provide three recommendations on tabular data to make it more accessible when importing on most analysis software or data management systems.

*   “Each row should represent a single observation (i.e., record) and each column should represent a single variable or type of measurement (i.e., field) (Borer et al. 2009, Strasser et al. 2011, 2012).
*   Every cell should contain only a single value (Strasser et al. 2012).
*   There should be only one column for each type of information” (pg 4)

Authors note that the most common violation of these rules is when the researcher uses a cross-tab structure. See “Recommended resources from this section” on resources to address this issue.

4.c - Use standard formats within cells  
Authors provide four recommendations:  

*   “_Be consistent_. For example, be consistent in your capitalization of words, choice of delimiters, and naming conventions for variables.
*   _Avoid special characters_. Most software for storing and analyzing data works best on plain text, and accents and other special characters can make it difficult to import your data (Borer et al. 2009, Strasser et al. 2012).
*   _Avoid using your delimiter in the data itself_ (e.g., commas in the notes filed of a comma-delimited file). This can make it difficult to import your data properly. This means that if you are using commas as the decimal separator (as is often done in continental Europe) then you should use a non- comma delimiter (e.g., a tab).
*   _When working with dates use the YYYY-MM-DD format_ (i.e., follow the ISO 8601 data standard).” (pg 4)

Recommended resources from this section:

*   For naming files recommends camel case or using underscores
*   Tools to restructure cross-tab data to avoid violating the rules in 4.b: “functions in Excel, R (e.g., melt() function in the R package reshape; Wick- ham 2007), and Python (e.g., melt() function in the Pandas Python module…” (pg 4).

_Tip 5 - Use good null valves_  
The authors discuss how working with missing or empty data valves is challenging. Therefore it is important for the original researcher to indicate this missing or empty value with a good null value. The authors recommend selecting a null value that works with as many different softwares as possible and won't cause errors in the analysis. They specifically recommend leaving the cell blank. They do note there are two challenges with using blacks, which are 1) blanks are unclear to the reader if the data is missing or simply overlooked during data entry or 2) Blanks can be confused for spaces and tabs in certain situations. But it is still the best overall.

The inserted chart includes all the authors recommendations for null values.

![](https://33333.cdn.cke-cs.com/kSW7V9NHUXugvhoQeFaf/images/b62b89fa85b9e9fe43eb1937ea6c56ae18b21ab1cec6fb98.png)

_Tip 6 - Make it easy to combine your data with other datasets_  
The authors recommend using contextual data to make combining your dataset easier with other datasets. Contextual data is typically data collected in the field, but not necessarily a variable (eg location). They recommend including this data in either the Metadata or as its own column.

The authors provide a lot of information in this section specific to the fields of ecology and biology. It might benefit the reader to check their own field regarding the inclusion and formatting of contextual data.

_Tip 7 - Perform Basic Quality Control_  
The authors note that quality control is important, because it helps decrease the chances of making a mistake in your own work and makes your dataset more usable in the future. The authors note there are both basic and advanced quality control checks that individuals can run. Advanced quality control checks involve more automation; python and R can assist. The authors provide three recommendations for basic quality control:  

*   Check to make sure there are no non-numeric values in a column that should only contain numeric values
*   Confirm that empty cells are not missing a data entry and are actually representing missing data. Indicate the missing data by using the appropriate null.
*   “Check for consistency in unit of measurement, data type (e.g., numeric, character), naming scheme (e.g., taxonomy, location), etc.” (pg 7).

Lastly, the authors recommend having someone review your data, in the same way you may ask someone to review your writing.

_Tip 8 - Use an established repository_  
“For data sharing to be effective, data should be easy to find, accessible, and stored where it will be preserved for a long time (Kowalczyk and Shankar 2011).” (pg 7). Therefore store your data in a repository.  
The authors recommend looking at where other researchers in your discipline publish their data. This will help you with identifying what repositories to use and what the standards are. The authors also suggest that you should look for repositories that have clear and easy to follow instructions and standards around citing work.  
Recommend resources for finding repositories:

*   http://databib.org
*   http://re3data.org.

_Tip 9 - Use an established and open license_  
Include a license with your data that's clear on the "... the rights and responsibilities of both the people providing the data and the people using it ..." and make the licenses as open as possible (pg7). Even minor restrictions have major consequences 2 additional resources for reading. 

Recommended resources for licenses:

*   Panton Principles
*   Creative Commons Zero (CCB)

_Conclusion_  
Data sharing can be transformative for the field of science and therefore there is growing interest in it. More funders are requesting for data to be shared. However, the shift will not be impactful, if others cannot use your data. The provided tips in this article should improve your data and make it more usable for others. The authors also emphasize that these tips are also just general better data practices. These tips can be implemented at any point in the project, but its best to start using them at the start.


## Libes, D. (1989). Choosing a Name for your Computer. Integrated Systems Group, National Institute of Standards and Technology, Gaithersburg, MD.

**Key Words:**

*   Domain name system
*   Naming conventions
*   Computer administration
*   Computer network management

  
**Important note:** This article begins with a Gary Larson cartoon, which should be enough encouragement for anyone to dive right in.  
 

**Summary:**

Naming (e.g., computers, networks, files, etc.) feels like it should be intuitive, but developing good naming conventions requires some thought. Libes outlines what NOT to do:

*   Don’t overload other terms already in common use.
*   Don’t choose a name after a project unique to that machine.
*   Don’t use your own name.
*   Don’t use long names.
*   Avoid alternate spellings.
*   Avoid domain names.
*   Avoid domain-like names.
*   Don’t use antagonistic or otherwise embarrassing names.
*   Don’t use digits at the beginning of the name.
*   Don’t use non-alphanumeric characters in a name.
*   Don’t expect case to be preserved.
*   Use words/names that are rarely used.
*   Use theme names.
*   Use real words.
*   Don’t worry about reusing someone else’s hostname.
*   There is always room for an exception.

Fundamentally, Libes suggests choosing one’s computer name wisely.

_Related reading:_ Mockapetris, P., "Domain Names - Concepts and Facilities", RFC 1034, Information Sciences Institute, CA, November 1987.

## Nosek, et al., 2015

  
Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G., Contestabile, M., Dafoe, A., Eich, E., Freese, J., Glennerster, R., Goroff, D., Green, D. P., Hesse, B., Humphreys, M., . . . Yarkoni, T. (2015). Promoting an open research culture. Science, 348(6242), 1422-1425. https://doi.org/10.1126/science.aab2374

The Transparency and Openness Promotion (TOP) guidelines aim to enhance scientific rigor through eight standards, each with three levels of implementation. The standards include: **citation standards** that extend citation norms to data and code; **replication standards** which recognize the importance of replication studies; and **design standards** that encourage transparent reporting of research methods. The guidelines also include standards for **research materials, data, and analytic methods** sharing that promote accessibility of research components. Additionally, the guidelines address **preregistration of studies and analysis plans**, which help distinguish between exploratory and confirmatory research.  

The levels of implementation increase in stringency, allowing journals to adopt guidelines gradually.  

- **Level 1** focuses on encouraging open practices  
- **Level 2** requires specific actions  
- **Level 3** includes independent reproduction of reported results

## Kontokosta, C. E. (2021)

https://journals.sagepub.com/doi/full/10.1177/0739456X18793716

The author emphasizes the importance of reproducible workflows in ensuring the quality and efficiency of science. The author suggests that reproducibility can be achieved by providing detailed documentation of the data sources, analytical methods, and software used in the study, as well as making the data and code publicly available.

### **Definitions**

The term "**reproducible**" in the context of research and data analysis typically refers to the ability to recreate or replicate the results of a study or analysis using the same data and methods. The author may define reproducible as ensuring that the processes, methodologies, and analyses conducted in a study are transparent and well-documented so that other researchers can replicate the findings. This transparency allows for verification of results and promotes trust in the research outcomes..

The author discusses the **workflow** in the context of an urban informatics project, emphasizing the importance of understanding the system in which the problem originates and the criticality of problem definition and understanding the problem context in developing usable solutions (Page 388)

**Quality of science** is addressed in terms of the interpretability versus accuracy trade-off in machine learning algorithms, where the author highlights the importance of building trust in data-driven decision making and the need for end-users to understand how the output was produced

### **Ways to measure**

To measure the **workflow** in the article, you can assess the level of transparency in documenting data sources and analytical methods, evaluate the reproducibility of the research findings by checking if data and code are made publicly available, analyze the efficiency of the workflow in terms of time and resource savings, examine the collaboration facilitated by reproducible workflows, and consider how the author emphasizes the importance of reproducibility for enhancing the quality and reliability of scientific research.

![](https://lh7-us.googleusercontent.com/3-18rBDb78BZGqj2zi8Ou9ZZ3T_ISIJtY73Zevn1vX79Q9uNAz8hE7EG1UPQOwfI7ImLW1KhI44Qk79qrGCtKfqcqoc0AWUYdJhoCEfjBOl19v_e2kVMVOHTMWB6z2X8DwwTGtTZphwwmc4Q0sB1EvA)

In **quality of life**, one can evaluate the robustness of the methodology employed, the transparency of the research process in documenting data sources and analytical methods, the reproducibility of the findings through publicly available data and code, the impact and contribution of the research to the field of urban planning, and the interdisciplinary approach taken to address complex urban challenges. Assessing these aspects will provide insights into the rigor, transparency, significance, and interdisciplinary nature of the scientific research presented in the article.

## Freese, J. (2007).

[https://journals.sagepub.com/doi/10.1177/0049124107306659](https://journals.sagepub.com/doi/10.1177/0049124107306659)

### **Research question**

Realizing that the credibility of quantitative social science is from the repeatable research results. The economic profession has been taking measures to improve the repeatability. With the development of computing and programming, quantitative social analysis becomes drastically complex. The lack of detail and repeatability could reduce the credibility of quantitative social science. The internet makes it possible for researchers to increase the transparency of research, thus increasing the credibility. By disputing six objections, the author argues that replication could benefit and increase the credibility of quantitative social science, and the cost is acceptable.

ASA: American Sociological Association (ASA) code of ethics, which is just that sociologists should ‘‘permit’’ attempts to verify results after their publication.

AER policy: “Papers could be published _only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication._”

### **Definitions**

**Reproducible** refers to the ability to replicate or reproduce the results of a study using the same data and procedures. It involves making all necessary information, such as data, code, and materials, available to others to verify the presented results.

**Workflow** is defined as the sequence of steps involved in conducting a research study, from data collection to analysis and reporting of results. Freese emphasizes the importance of documenting and sharing this workflow to enhance transparency and replicability in social science research.

These are the skills required to manage and document the workflow of a research project effectively. Researchers are encouraged to develop these skills to ensure that their work is transparent and reproducible.

**Quality of science** means to the credibility and reliability of scientific research. Freese argues that increasing transparency and replication standards in sociology can enhance the quality of science by allowing for the verification of results and promoting the sharing of research materials for further analysis and extension by other researchers

### **Ways to measure**

The **reproducibility** of research findings can be measured by evaluating the extent to which researchers make their data and code available for replication purposes. Compliance with replication policies and standards, such as depositing materials in online archives, can also indicate the level of reproducibility in quantitative social science research. Transparency in reporting methods and decisions made during the research process is crucial for assessing reproducibility. By examining data availability, code sharing, transparency in methods, and adherence to replication policies, the reproducibility of research in sociology can be effectively measured.

Measuring the "**Workflow**" in research involves evaluating the completeness and clarity of documentation, adherence to reproducibility standards, transparency in methods, and compliance with best practices. Researchers can assess the quality of their workflow by documenting data collection, cleaning procedures, analysis steps, and result interpretation. Transparency in reporting methods and adherence to best practices, such as version control and clear communication of the workflow, are essential for ensuring the robustness and reproducibility of research in quantitative social science.: (1) Is the original data provided? (2) Is the code stored?(3) Did the researcher save the syntax, when they use the GUI? (4) Did the researcher document the version information on the data?

## Arribas-Bel, D., de Graaff, T., & Rey, S. J. (2017).

Arribas-Bel, D., de Graaff, T., & Rey, S. J. (2017). Looking at John Snow’s cholera map from the twenty-first century: A practical primer on reproducibility and open science. In R. Jackson & P. Schaeffer (Eds.), Regional research frontiers - Vol. 2: Methodological advances, regional systems modeling, and open sciences (pp. 283–305). Springer. https://doi.org/10.1007/978-3-319-50590-9_17

**Authors:** Arribas-Bel, D., de Graaff, T., & Rey, S. J. (2017)
**Source:** Regional Research Frontiers - Vol. 2: Methodological Advances, Regional Systems Modeling, and Open Sciences

### 1. Introduction
**Objective:**
*   Demonstrates the importance of reproducibility and Open Science in regional analysis.
*   Uses John Snow’s cholera map as a case study to showcase modern spatial analysis techniques.
*   Provides hands-on code examples in R and Python using Jupyter Notebooks.

### 2. Data Import and Setup
**Required Libraries:**
# Python Libraries
import rpy2.robjects as robjects
import rpy2.robjects.conversion
import rpy2.interactive as r
import rpy2.interactive.packages
import seaborn as sns
import pandas as pd
import pysal as ps
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt

### 3. Data Loading and Visualization
**Extract John Snow’s Cholera Data in R:**
# Load Data
r.packages.importr('HistData')
r.packages.importr('ggplot2')

# Plot Cholera Death Locations
Snow_plot <- ggplot(Snow.deaths, aes(x = x, y = y)) +
  geom_point(data = Snow.deaths, aes(x = x, y = y), col = "red", pch = 19, cex = 1.5) +
  geom_point(data = Snow.pumps, aes(x = x, y = y), col = "black", pch = 17, cex = 4) +
  geom_text(data = Snow.pumps, aes(label = label, x = x, y = y+0.5)) +
  xlim(6, 19.5) + ylim(4, 18.5) +
  geom_path(data = Snow.streets, aes(x = x, y = y, group = street), col = "gray40") +
  ggtitle("Pumps and Cholera Deaths in 19th Century London") +
  theme(panel.background = element_rect(fill = "gray85"))

### 4. Spatial Analysis and Modeling
**Load GIS Data:**
pumps = gpd.read_file('data/Pumps.shp')   # Pump locations
blocks = gpd.read_file('data/polys.shp')  # Building blocks
js = gpd.read_file('data/streets_js.shp') # Street network

### 5. Creating a Spatial Weights Matrix
**Contiguity-Based Spatial Weights:**
# Load the network
ntw = ps.Network('data/streets_js.shp')

# Create the spatial weights matrix
w = ntw.contiguityweights(graph=False)

# Rename IDs to match street segments
w.remap_ids(js['segIdStr'])

# Row-standardize the matrix
w.transform = 'R'

### Conclusion
**Key Takeaways:**
*   Reproducibility and Open Science Improve Research Efficiency
Automating workflows saves time and ensures transparency.
Open Science principles enhance collaboration and credibility.
*   Modern Spatial Analysis Confirms John Snow’s Findings
Using Python, R, and GIS tools, the study validated cholera clusters around a contaminated pump.
Spatial autocorrelation (Moran’s I, LISA) confirmed statistically significant clustering.
*   Adopting Reproducible Workflows Benefits Science
Version control (Git), Jupyter Notebooks, and Markdown enhance research accessibility.
Open research leads to faster knowledge dissemination and greater scientific impact.


## Playford et al. (2016)

Playford, C. J., Gayle, V., Connelly, R., & Gray, A. J. (2016). Administrative social science data: The challenge of reproducible research. Big Data & Society, 3(2). https://doi.org/10.1177/2053951716684143

Playford et al. (2016) address how reproducible workflow skills affect the quality and efficiency of science by emphasizing the importance of a planned and organized workflow, particularly when using administrative data. The authors argue that sharing research code is essential for achieving reproducibility, which enhances the accuracy and validity of analyses, as well as facilitating further testing of research. They advocate for the use of version control software like Git to improve code sharing and make research more reproducible. **The paper highlights that a well-planned workflow and the sharing of research code are critical for improving the overall quality and efficiency of social science research, making it more transparent and reproducible**. The authors also note that the use of syntax files (as opposed to graphical user interfaces) is imperative for reproducibility and transparency.

### **Challenges & Solutions**

- Challenge 1: Lack of code sharing and documentation. Social scientists often don't share their analysis code, which makes research difficult to reproduce. Using point-and-click methods instead of syntax also means that analysis steps aren't recorded.

  Solution 1: Share all research code (syntax files) and any code used to get or combine data. Always use syntax files instead of graphical interfaces to make sure the analysis 
  is recorded.

- Challenge 2: Restrictions on data access and retention. Access restrictions and short data retention periods limit the ability to share data and code.

  Solution 2: Use meta-information and share code to help make research reproducible. Work toward policies that keep data and research code for a long time.

- Challenge 3: Working with changing data. Data from administrative systems can change over time, so it's hard to know exactly which data were used.
  
  Solution 3: Document the code and time parameters used to get the data from the dynamic system.

- Challenge 4: Limited exploratory data analysis when using administrative data.

  Solution 4: Develop better tools to explore this kind of data.

- Challenge 5: Lack of version control. Without version control, it's hard to track changes in code or go back to older versions. Using personal websites or repositories is not 
  ideal.

  Solution 5: Use version control systems like Git to manage code changes. Also, use Research Objects (ROs) to organize all research outputs (data, code, documentation).

## Arribas-Bel, D., de Graaff, T., & Rey, S. (2014). 

Arribas-Bel, D., de Graaff, T., & Rey, S. J. (2017). Looking at John Snow’s cholera map from the twenty-first century: A practical primer on reproducibility and open science. In R. Jackson & P. Schaeffer (Eds.), Regional research frontiers - Vol. 2: Methodological advances, regional systems modeling, and open sciences (pp. 283–305). Springer. 

### Relevance to Reproducibility Research
*   Definitions (p. 284-285): Clarifies key terms—workflow, reproducibility, and Open Science.
*   Challenges (p. 285-286): Discuss obstacles like proprietary data, lack of training, and time costs.
*   Social norms vs. individual goals (p. 304-305): Explores tensions between academic competition and Open Science.
*   Software & coding (p. 286-287, 289-293): Demonstrates data analysis, documentation, and workflow automation using modern computational tools.
*   Impact on science quality & efficiency (p. 287-288, 304-305): Shows how reproducible workflows enhance transparency, credibility, and efficiency.
### Conclusion
**Key Takeaways:**
*   Reproducibility and Open Science Improve Research Efficiency
Automating workflows saves time and ensures transparency.
Open Science principles enhance collaboration and credibility.
*   Modern Spatial Analysis Confirms John Snow’s Findings
Using Python, R, and GIS tools, the study validated cholera clusters around a contaminated pump.
Spatial autocorrelation (Moran’s I, LISA) confirmed statistically significant clustering.
*   Adopting Reproducible Workflows Benefits Science
Version control (Git), Jupyter Notebooks, and Markdown enhance research accessibility.
Open research leads to faster knowledge dissemination and greater scientific impact.

### Ziemann, M., Poulain, P., & Bora, A. (2023). The five pillars of computational reproducibility: bioinformatics and beyond. Briefings in Bioinformatics, 24(6).
This review article addresses the persistent reproducibility crisis in bioinformatics and computational research and proposes a practical framework, the "Five Pillars of Computational Reproducibility." The authors synthesize evidence from previous reproducibility studies and real-world failure cases, highlighting that irreproducibility remains widespread and has serious consequences, particularly in data-intensive biomedical research. They define five interconnected pillars: readable programming, code version control and sharing, control of the computational environment, persistent data sharing, and comprehensive documentation.


