# Quality and Efficiency of Science

The table below consolidates the definitions of **quality of science** and **efficiency of science** found across the readings in the [annotated bibliography](00_URSC645_AnnotatedBibliography.md).

> **Note to future students:** 
- Identify if the definition is a direct quote
- If it is not a direct quote, check to see if the text can be replaced with a direct quote. Direct quotes are preferred over summaries.
- All direct quotes need to be inside " marks and include the page number (p. ###)
- Add page numbers for direct quotes or to the part of the text that is summarized.
- Check for accuracy; current summary created using Claude Opus 4.6 

| In-Text Citation | Quality of Science Defined | Efficiency of Science Defined |
| --- | --- | --- |
| Freese (2007) | Credibility and reliability of scientific research. Increasing transparency and replication standards can enhance the quality of science by allowing for the verification of results and promoting the sharing of research materials for further analysis and extension by other researchers. | Not explicitly addressed |
| White et al (2013) | Measured by data accuracy and the ability to catch typographical or recorder errors before sharing. | Measured by the ability of scientists to spend less time "deciphering and cleaning up data" and more time doing actual science. |
| Gentzkow and Shapiro (2014) Ch. 1 | Measured by the avoidance of "inconsistent samples" and the ability to build upon previous work without having to rewrite code from scratch. | Measured by time spent on actual research questions versus time wasted "wrestling with... messy code" or "reverse-engineering" old project directories. |
| Gentzkow and Shapiro (2014) Ch. 2 | Measured by the existence of an "authoritative definition" of what the numbers in a paper actually are, replacing the fallible memory of the researcher. | Measured by the ability to rerun a thousand-step process hundreds of times without manual effort; the authors note that automation "pays big dividends" as the analysis is refined. |
| Playford et al (2016) | Sharing research code enhances the accuracy and validity of analyses, as well as facilitating further testing of research. | A well-planned workflow and the sharing of research code are critical for improving the overall efficiency of social science research. |
| Hoelter et al (2008) | Asking students to replicate an article of interest is a valuable way to socialize students to the expectations and practices of the discipline, teaches them the norms of scientific transparency, and helps them get published early in their careers (Hoelter et al. p. 21). | Using online tools helps students to begin direct replication practices by allowing them to ask research questions that they are interested in and finding an article in a sociological journal (Hoelter et al. p. 21). |
| Munafo et al (2017) | By making analytic processes explicit and verifiable, reproducible workflow skills improve the quality of scientific findings through increased credibility, robustness, and transparency. | Reproducible workflows enhance research efficiency by reducing duplicated effort, enabling faster verification of results, and supporting cumulative knowledge building rather than repeated reinvention. |
| Kontokosta (2021) | Addressed in terms of the interpretability versus accuracy trade-off in machine learning algorithms; the importance of building trust in data-driven decision making and ensuring end-users understand how the output was produced. | Efficiency in terms of time and resource savings through reproducible workflows. |

### Summary: How Have Quality and Efficiency of Science Evolved?

**Quality of Science** has evolved from a focus on **credibility and accuracy** to an emphasis on **transparency and trust** in scientific processes.

Early definitions (2007–2013) approach quality primarily through the lens of **data integrity and research credibility**. Freese (2007) defines it in terms of the credibility and reliability of research findings, achieved through transparency and replication standards. White et al (2013) operationalize it narrowly: quality is measured by data accuracy and error detection. At this stage, quality is seen as a *product* of careful data handling.

**A shift occurs with Gentzkow and Shapiro (2014)**, who reframe quality not as a data property but as a workflow property. In Chapter 1, they define quality by outcomes: avoiding inconsistent samples and enabling future scholars to build on previous work without rewriting code. In Chapter 2, they deepen this further—quality exists when there is an explicit, reproducible definition of what the numbers mean, replacing reliance on the researcher's fallible memory. This represents a critical evolution: quality becomes tied not just to *accuracy* but to *verifiability* and *transferability*.

**Post-2014 definitions reflect this broadened understanding.** Playford et al (2016) link quality to code sharing and validity of analyses, recognizing that quality depends on the entire workflow being transparent and reproducible. Munafo et al (2017) further elevate this, framing quality as arising from reproducible workflows that make analytic processes "explicit and verifiable," thus improving "credibility, robustness, and transparency." Quality is no longer just about getting the right answer; it's about being able to prove how you got it.

**Most recent work (2021)** adds an additional dimension: **trust and interpretability.** Kontokosta (2021) emphasizes that quality involves not just accuracy but also interpretability—end-users must understand how outputs were produced. This reflects a maturation of the field's understanding: quality of science now encompasses both the technical integrity of analyses and the communicability of methods to others.

---

**Efficiency of Science** has evolved from a focus on **time savings** to an emphasis on **systemic knowledge building**.

Early definitions (2007–2013) view efficiency as **labor savings**. Freese (2007) does not explicitly address efficiency. White et al (2013) define it narrowly: spending less time cleaning data means more time doing "actual science"—efficiency is time reclaimed from drudgery. This framing assumes efficiency is simply about saving labor on data preparation.

**Gentzkow and Shapiro (2014)** expand this significantly. In Chapter 1, they show that efficiency is lost not just in data cleaning but in the entire workflow—time spent reverse-engineering old projects, wrestling with messy code, determining what was done. In Chapter 2, they quantify the payoff: automation allows researchers to rerun analyses hundreds of times "without manual effort" and creates "big dividends" as analyses are refined. Efficiency is no longer just about *initial* time savings but about *enabling iteration* and *scaling* analyses.

**Post-2014 work emphasizes systemic benefits.** Playford et al (2016) connect efficiency to workflow planning—a well-organized workflow improves overall efficiency. Hoelter et al (2008) reframe efficiency not as time saved but as *capacity enabled*—tools that allow students to ask their own research questions and find relevant articles expand the scope of what can be accomplished. Munafo et al (2017) make the most expansive argument: reproducible workflows enhance efficiency by "reducing duplicated effort, enabling faster verification," and crucially, "supporting cumulative knowledge building rather than repeated reinvention." Efficiency becomes a **collective, systemic property**—not just about individual researchers working faster, but about the entire scientific enterprise avoiding redundant effort.

**Key areas of agreement:**
- Quality requires transparency and verifiability, not just accuracy
- Efficiency is enhanced when workflows are automated and systematized
- Both quality and efficiency depend on sharing code, data, and clear documentation
- Both concepts have evolved from individual-level to system-level properties

**Key areas of evolution or complexity:**
- Quality has expanded from "correct numbers" to "correct numbers *that others can verify and build upon*"
- Efficiency has shifted from "faster individual research" to "faster knowledge accumulation across the scientific community"
- The trade-off between interpretability and accuracy (Kontokosta 2021) introduces a complication—maximum technical accuracy may not maximize scientific quality if results cannot be understood or trusted by end-users

In summary, both definitions have matured from **operational metrics** (time saved, errors caught) to **epistemic principles** (what makes findings credible and cumulative). The field increasingly recognizes that quality and efficiency are not just about individual research projects but about the *infrastructure that enables reliable, replicable, and cumulative* science.

---

AI Use Statement: 

This page was generated on **February 10, 2026** using **GitHub Copilot (Claude Haiku 4.5)** through a **VS Code GitHub Copilot Pro Education License**.

The following prompts were used in sequence:

1. **Extract and tabulate definitions:** "Review the annotated bibliography file and consolidate the definitions of 'quality of science' and 'efficiency of science' into a new file called 01_URSC645_QualityAndEfficiencyOfScience.md. Make a table with columns for In-Text Citation, Quality of Science Defined, and Efficiency of Science Defined. Populate from the annotated bibliography."
2. **Sort the table:** "Sort this table by year and then author last name."
3. **Summarize and analyze definitions:** "Create a follow-up section that summarizes how both quality of science and efficiency of science have been defined and evolved over time. Include sections for key areas of agreement and key areas of evolution or complexity."
4. **Add AI use statement:** "Add an AI use statement to the bottom of this page describing the use of VS Code GitHub Copilot Pro Education License and summarizing the prompts used."

The AI reviewed the full contents of [00_URSC645_AnnotatedBibliography.md](00_URSC645_AnnotatedBibliography.md) to identify and extract definitions of "quality of science" and "efficiency of science" from each article entry, then consolidated them into the table above. The summary section synthesizing evolution, agreements, and complexities over time was generated by the AI based on the extracted definitions.
